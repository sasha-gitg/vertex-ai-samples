{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/vertex-ai-samples/notebooks/community/feature_store/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/inardini/vertex-ai-samples/blob/main/vertex-ai-samples/notebooks/community/feature_store/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FZeBEwdXS4d"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Imagine you are a member of the Data Science team working on the same Mobile Gaming application reported in the [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml) blog post. \n",
    "\n",
    "Business wants to use that information in real-time to monetize it by implementing a conditional ads system. In particular, each time a user plays with the app, they want to display ads depending on the customer demographic,  behavioral information and the resulting propensity of return. Of course, the new application should work with a minimum impact on the user experience. \n",
    "\n",
    "Last year, Google Cloud announced Vertex AI, a managed machine learning (ML) platform that allows data science teams to accelerate the deployment and maintenance of ML models. One of the platform building blocks is Vertex AI Feature store which provides a managed service for low latency scalable feature serving. Also it is a centralized feature repository with easy APIs to search & discover features and feature monitoring capabilities to track drift and other quality issues. \n",
    "\n",
    "In this notebook, we will show how the role of Vertex AI Feature Store in a ready to production scenario when the user's activities within the first 24 hours of first user engagement and the gaming platform would consume in order to offer conditional ads. Below you can find the high level picture of the system\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./assets/1_mobile_gaming_architeture.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset is the public sample export data from an actual mobile game app called \"Flood It!\" (Android, iOS)\n",
    "\n",
    "### Objective\n",
    "\n",
    "In the following notebook, you will learn how Vertex AI Feature store\n",
    "\n",
    "1.   Provide a centralized feature repository with easy APIs to search & discover features and fetch them for training/serving. \n",
    "\n",
    "2.   Simplify deployments of models for Online Prediction, via low latency scalable feature serving.\n",
    "\n",
    "3.   Mitigate training serving skew and data leakage by performing point in time lookups to fetch historical data for training.\n",
    "\n",
    "**Notice that we assume that already know how to set up a Vertex AI Feature store. In case you are not, please check out [this detailed notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/gapic-feature-store.ipynb).**\n",
    "\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as {XGBoost, AdaNet, or TensorFlow Hub TODO: Replace with relevant packages for the tutorial}. Use the latest major GA version of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzEo6DeE2GOP",
    "outputId": "be913c86-0ee1-40b2-dc65-6339f93bf9ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.0.4)\n",
      "Collecting git+https://github.com/googleapis/python-aiplatform.git@main\n",
      "  Cloning https://github.com/googleapis/python-aiplatform.git (to revision main) to /tmp/pip-req-build-zyb5z4mi\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/googleapis/python-aiplatform.git /tmp/pip-req-build-zyb5z4mi\n",
      "  Resolved https://github.com/googleapis/python-aiplatform.git to commit 74ffa19e7d540f6bb5f21d2335c2a5d23cc49ee2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.11.0) (2.4.0)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.11.0) (1.19.9)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.11.0) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.11.0) (2.0.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.11.0) (2.24.0)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.4.1-py2.py3-none-any.whl (402 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (59.8.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (2.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (1.54.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (2.27.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (3.19.4)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (1.43.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.11.0) (2.2.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.11.0) (2.1.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.11.0) (0.12.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.11.0) (3.0.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (4.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.11.0) (1.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (2.0.11)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.11.0) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.11.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.11.0) (2.21)\n",
      "Installing collected packages: google-cloud-resource-manager\n",
      "Successfully installed google-cloud-resource-manager-1.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade pip\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform==1.11.0 -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} git+https://github.com/googleapis/python-aiplatform.git@main\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-bigquery==2.24.0 -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} --upgrade lightgbm==3.3.2 -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component). \n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM1iC_MfAts1",
    "outputId": "6b7c688e-bae1-4ab6-9061-ebc275ebd084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  inardini-playground\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"inardini-playground\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEjRdjxBuDsi",
    "outputId": "a43c6396-8cfb-4e5e-91e7-c7d9041eda98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID #change it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://mobile-gaming\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIq7R4HZCfIc",
    "outputId": "8932ffa1-ce12-4cd3-f3cb-1bdc8145e34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mobile-gaming/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mobile-gaming' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "994afa65eaa2"
   },
   "source": [
    "Run the following cell to grant access to your Cloud Storage resources from Vertex AI Feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psP1rPU9TRnX",
    "outputId": "8b2ace91-05e2-417b-c1ae-e52fcebd20d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccessDeniedException: 403 309823771116-compute@developer.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\n"
     ]
    }
   ],
   "source": [
    "! gsutil uniformbucketlevelaccess set on $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhOb7YnwClBb",
    "outputId": "c9a81b63-5b01-436c-9e61-073136c82b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccessDeniedException: 403 309823771116-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utO91mebwEmw"
   },
   "source": [
    "### Create a Bigquery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3G2BXqswb_J",
    "outputId": "a76e0231-fda1-41a0-dc6c-7ccc3a8bc89a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'inardini-playground:Mobile_Gaming'\n",
      "already exists.\n"
     ]
    }
   ],
   "source": [
    "BQ_DATASET = \"Mobile_Gaming\"  # @param {type:\"string\"}\n",
    "LOCATION = \"US\"\n",
    "\n",
    "!bq mk --location=$LOCATION --dataset $PROJECT_ID:$BQ_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Vertex AI and its Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import Feature, Featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgP_N3MSpnd3"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b_k2ejxQsET8"
   },
   "outputs": [],
   "source": [
    "# Data Engineering and Feature Engineering\n",
    "FEATURES_TABLE = \"wide_features_table\"  # @param {type:\"string\"} \n",
    "TODAY = \"2018-10-03\"  \n",
    "TOMORROW = \"2018-10-04\"\n",
    "FEATURES_TABLE_TODAY = f\"wide_features_table_{TODAY}\"\n",
    "FEATURES_TABLE_TOMORROW = f\"wide_features_table_{TOMORROW}\"\n",
    "FEATURESTORE_ID = \"mobile_gaming\"  # @param {type:\"string\"}\n",
    "ENTITY_TYPE_ID = \"user\"\n",
    "\n",
    "# Vertex AI Feature store\n",
    "ONLINE_STORE_NODES_COUNT = 3\n",
    "ENTITY_ID = \"user\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "FEATURE_TIME = \"user_first_engagement\"\n",
    "ENTITY_ID_FIELD = \"user_pseudo_id\"\n",
    "BQ_SOURCE_URI_TODAY = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_TODAY}\"\n",
    "BQ_SOURCE_URI_TOMORROW = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_TOMORROW}\"\n",
    "GCS_DESTINATION_OUTPUT_URI = f\"{BUCKET_URI}/data/features/train_features_{TIMESTAMP}\"\n",
    "SERVING_FEATURE_IDS = {\"user\": [\"*\"]}\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{TIMESTAMP}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Vertex AI Training \n",
    "BASE_CPU_IMAGE='us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest'\n",
    "DATASET_NAME = f\"churn_mobile_gaming_{TIMESTAMP}\"\n",
    "TRAIN_JOB_NAME = f\"xgb_classifier_training_{TIMESTAMP}\"\n",
    "MODEL_NAME = f\"churn_xgb_classifier_{TIMESTAMP}\"\n",
    "MODEL_PACKAGE_PATH = \"train_package\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "TRAINING_REPLICA_COUNT=1\n",
    "DATA_PATH = f\"{GCS_DESTINATION_OUTPUT_URI}/000000000000.csv\".replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_DIR = f\"{BUCKET_URI}/model/{TIMESTAMP}\".replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Vertex AI Prediction\n",
    "DESTINATION_URI = f\"{BUCKET_URI}/model/{TIMESTAMP}\"\n",
    "VERSION = \"v1\"\n",
    "SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    "ENDPOINT_NAME = \"mobile_gaming_churn\"\n",
    "DEPLOYED_MODEL_NAME = f\"churn_xgb_classifier_{VERSION}\"\n",
    "MODEL_DEPLOYED_NAME = \"churn_xgb_classifier_v1\"\n",
    "SERVING_MACHINE_TYPE = \"n1-highcpu-4\"\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTjqqE0jafnn"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9dJ_TGTVak7u"
   },
   "outputs": [],
   "source": [
    "def run_bq_query(query: str):\n",
    "    \"\"\"\n",
    "    An helper function to run a BigQuery job\n",
    "    Args:\n",
    "        query: a formatted SQL query\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        job = bq_client.query(query)\n",
    "        _ = job.result()\n",
    "    except RuntimeError as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "def upload_model(\n",
    "    display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    artifact_uri: str,\n",
    "    sync: bool = True,\n",
    ") -> vertex_ai.Model:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        display_name: The name of Vertex AI Model artefact\n",
    "        serving_container_image_uri: The uri of the serving image\n",
    "        artifact_uri: The uri of artefact to import\n",
    "        sync:\n",
    "\n",
    "    Returns: Vertex AI Model\n",
    "\n",
    "    \"\"\"\n",
    "    model = vertex_ai.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        sync=sync,\n",
    "    )\n",
    "    model.wait()\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_endpoint(display_name: str) -> vertex_ai.Endpoint:\n",
    "    \"\"\"\n",
    "    An utility to create a Vertex AI Endpoint\n",
    "    Args:\n",
    "        display_name: The name of Endpoint\n",
    "\n",
    "    Returns: Vertex AI Endpoint\n",
    "\n",
    "    \"\"\"\n",
    "    endpoint = vertex_ai.Endpoint.create(display_name=display_name)\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model: vertex_ai.Model,\n",
    "    machine_type: str,\n",
    "    endpoint: vertex_ai.Endpoint = None,\n",
    "    deployed_model_display_name: str = None,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    ") -> vertex_ai.Model:\n",
    "    \"\"\"\n",
    "    An helper function to deploy a Vertex AI Endpoint\n",
    "    Args:\n",
    "        model: A Vertex AI Model\n",
    "        machine_type: The type of machine to serve the model\n",
    "        endpoint: An Vertex AI Endpoint\n",
    "        deployed_model_display_name: The name of the model\n",
    "        min_replica_count: Minimum number of serving replicas\n",
    "        max_replica_count: Max number of serving replicas\n",
    "        sync: Whether to execute method synchronously\n",
    "\n",
    "    Returns: vertex_ai.Model\n",
    "\n",
    "    \"\"\"\n",
    "    model_deployed = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model_deployed.wait()\n",
    "\n",
    "    print(model_deployed.display_name)\n",
    "    print(model_deployed.resource_name)\n",
    "    return model_deployed\n",
    "\n",
    "\n",
    "def endpoint_predict_sample(\n",
    "    instances: list, endpoint: vertex_ai.Endpoint\n",
    ") -> vertex_ai.models.Prediction:\n",
    "    \"\"\"\n",
    "    An helper function to get prediction from Vertex AI Endpoint\n",
    "    Args:\n",
    "        instances: The list of instances to score\n",
    "        endpoint: An Vertex AI Endpoint\n",
    "\n",
    "    Returns:\n",
    "        vertex_ai.models.Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    print(prediction)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def simulate_prediction(\n",
    "    endpoint: vertex_ai.Endpoint, online_sample: dict\n",
    ") -> vertex_ai.models.Prediction:\n",
    "    \"\"\"\n",
    "    An helper function to simulate online prediction with customer entity type\n",
    "        - format entities for prediction\n",
    "        - retrive static features with a singleton lookup operations from Vertex AI Feature store\n",
    "        - run the prediction request and get back the result\n",
    "    Args:\n",
    "        endpoint:\n",
    "        online_sample:\n",
    "\n",
    "    Returns:\n",
    "        vertex_ai.models.Prediction\n",
    "    \"\"\"\n",
    "    online_features = pd.DataFrame.from_dict(online_sample)\n",
    "    entity_ids = online_features[\"entity_id\"].tolist()\n",
    "\n",
    "    customer_aggregated_features = user_entity_type.read(\n",
    "        entity_ids=entity_ids,\n",
    "        feature_ids=[\n",
    "            \"cnt_user_engagement\",\n",
    "            \"cnt_level_start_quickplay\",\n",
    "            \"cnt_level_end_quickplay\",\n",
    "            \"cnt_level_complete_quickplay\",\n",
    "            \"cnt_level_reset_quickplay\",\n",
    "            \"cnt_post_score\",\n",
    "            \"cnt_spend_virtual_currency\",\n",
    "            \"cnt_ad_reward\",\n",
    "            \"cnt_challenge_a_friend\",\n",
    "            \"cnt_completed_5_levels\",\n",
    "            \"cnt_use_extra_steps\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    prediction_sample_df = pd.merge(\n",
    "        customer_aggregated_features.set_index(\"entity_id\"),\n",
    "        online_features.set_index(\"entity_id\"),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # prediction_sample = prediction_sample_df.to_dict(\"records\")\n",
    "    prediction_instance = prediction_sample_df.values.tolist()\n",
    "    print(prediction_instance)\n",
    "    prediction = endpoint.predict(prediction_instance)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MewDhNbrXf_N"
   },
   "source": [
    "# Setting the realtime scenario \n",
    "\n",
    "This section we will static features we want to fetch from Vertex AI Feature Store. In particular, we will cover the following steps:\n",
    "\n",
    "1. Identify users and the label feature, process demographic features and process behavioral features within the first 24 hours of app installation using **BigQuery**\n",
    "\n",
    "2. Set up the feature store\n",
    "\n",
    "3. Register features using **Vertex AI Feature Store** and the SDK.\n",
    "\n",
    "<img src=\"./assets/2_feature_store_ingestion.png\" width=\"60%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffd54e97270"
   },
   "source": [
    "## Initiate clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f48872501fb",
    "outputId": "511ef264-d905-4b21-d670-9c4c65809329"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmMWIpCwsET9"
   },
   "source": [
    "## Identify users and build your features\n",
    "\n",
    "The original dataset contains raw event data we cannot ingest in the feature store as they are. We need to pre-process the raw data in order to get user features. \n",
    "\n",
    "**Notice we simulate those transformations in different point of time (today and tomorrow).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8avYy5QOv02s"
   },
   "source": [
    "### Label, Demographic and Behavioral Transformations\n",
    "\n",
    "This section is based on the [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml?utm_source=linkedin&utm_medium=unpaidsoc&utm_campaign=FY21-Q2-Google-Cloud-Tech-Blog&utm_content=google-analytics-4&utm_term=-) blog article by Minhaz Kazi and Polong Lin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nabXJGAOsET9"
   },
   "outputs": [],
   "source": [
    "preprocess_sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}` AS\n",
    "WITH\n",
    "  # query to create label --------------------------------------------------------------------------------\n",
    "  get_label AS (\n",
    "  SELECT\n",
    "    user_pseudo_id,\n",
    "    user_first_engagement,\n",
    "    user_last_engagement,\n",
    "    # EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month,\n",
    "    # EXTRACT(DAYOFYEAR from TIMESTAMP_MICROS(user_first_engagement)) as julianday,\n",
    "    # EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek,\n",
    "\n",
    "    #add 24 hr to user's first touch\n",
    "    (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,\n",
    "\n",
    "    #churned = 1 if last_touch within 24 hr of app installation, else 0\n",
    "    IF (user_last_engagement < (user_first_engagement + 86400000000),\n",
    "        1,\n",
    "        0 ) AS churned,\n",
    "\n",
    "    #bounced = 1 if last_touch within 10 min, else 0\n",
    "    IF (user_last_engagement <= (user_first_engagement + 600000000),\n",
    "        1,\n",
    "        0 ) AS bounced,\n",
    "  FROM\n",
    "    (\n",
    "      SELECT\n",
    "      user_pseudo_id,\n",
    "      MIN(event_timestamp) AS user_first_engagement,\n",
    "      MAX(event_timestamp) AS user_last_engagement\n",
    "      FROM\n",
    "        `firebase-public-project.analytics_153293282.events_*`\n",
    "      WHERE event_name=\"user_engagement\"\n",
    "      GROUP BY\n",
    "        user_pseudo_id\n",
    "    )\n",
    "  GROUP BY 1,2,3),\n",
    "\n",
    "  # query to create class weights --------------------------------------------------------------------------------\n",
    "  get_class_weights AS (\n",
    "  SELECT\n",
    "    CAST(COUNT(*) / (2*(COUNT(*) - SUM(churned))) AS STRING) AS class_weight_zero,\n",
    "    CAST(COUNT(*) / (2*SUM(churned)) AS STRING) AS class_weight_one,\n",
    "  FROM\n",
    "    get_label\n",
    "    ),\n",
    "\n",
    "  # query to extract demographic data for each user ---------------------------------------------------------\n",
    "  get_demographic_data AS (\n",
    "  SELECT * EXCEPT (row_num)\n",
    "  FROM (\n",
    "    SELECT\n",
    "      user_pseudo_id,\n",
    "      geo.country as country,\n",
    "      device.operating_system as operating_system,\n",
    "      device.language as language,\n",
    "      ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num\n",
    "    FROM `firebase-public-project.analytics_153293282.events_*`\n",
    "    WHERE event_name=\"user_engagement\")\n",
    "  WHERE row_num = 1),\n",
    "\n",
    "  # query to extract behavioral data for each user ----------------------------------------------------------\n",
    "  get_behavioral_data AS (\n",
    "  SELECT\n",
    "    user_pseudo_id,\n",
    "    SUM(IF(event_name = 'user_engagement', 1, 0)) AS cnt_user_engagement,\n",
    "    SUM(IF(event_name = 'level_start_quickplay', 1, 0)) AS cnt_level_start_quickplay,\n",
    "    SUM(IF(event_name = 'level_end_quickplay', 1, 0)) AS cnt_level_end_quickplay,\n",
    "    SUM(IF(event_name = 'level_complete_quickplay', 1, 0)) AS cnt_level_complete_quickplay,\n",
    "    SUM(IF(event_name = 'level_reset_quickplay', 1, 0)) AS cnt_level_reset_quickplay,\n",
    "    SUM(IF(event_name = 'post_score', 1, 0)) AS cnt_post_score,\n",
    "    SUM(IF(event_name = 'spend_virtual_currency', 1, 0)) AS cnt_spend_virtual_currency,\n",
    "    SUM(IF(event_name = 'ad_reward', 1, 0)) AS cnt_ad_reward,\n",
    "    SUM(IF(event_name = 'challenge_a_friend', 1, 0)) AS cnt_challenge_a_friend,\n",
    "    SUM(IF(event_name = 'completed_5_levels', 1, 0)) AS cnt_completed_5_levels,\n",
    "    SUM(IF(event_name = 'use_extra_steps', 1, 0)) AS cnt_use_extra_steps,\n",
    "  FROM (\n",
    "    SELECT\n",
    "      e.*\n",
    "    FROM\n",
    "      `firebase-public-project.analytics_153293282.events_*` e\n",
    "    JOIN\n",
    "      get_label r\n",
    "    ON\n",
    "      e.user_pseudo_id = r.user_pseudo_id\n",
    "    WHERE\n",
    "      e.event_timestamp <= r.ts_24hr_after_first_engagement\n",
    "    )\n",
    "  GROUP BY 1)\n",
    "\n",
    "SELECT\n",
    "    PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(ret.user_first_engagement))) AS user_first_engagement,\n",
    "    # ret.month,\n",
    "    # ret.julianday,\n",
    "    # ret.dayofweek,\n",
    "    dem.*,\n",
    "    CAST(IFNULL(beh.cnt_user_engagement, 0) AS FLOAT64)  AS cnt_user_engagement,\n",
    "    CAST(IFNULL(beh.cnt_level_start_quickplay, 0) AS FLOAT64) AS cnt_level_start_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_end_quickplay, 0) AS FLOAT64) AS cnt_level_end_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_complete_quickplay, 0) AS FLOAT64) AS cnt_level_complete_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_reset_quickplay, 0) AS FLOAT64) AS cnt_level_reset_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_post_score, 0) AS FLOAT64) AS cnt_post_score,\n",
    "    CAST(IFNULL(beh.cnt_spend_virtual_currency, 0) AS FLOAT64) AS cnt_spend_virtual_currency,\n",
    "    CAST(IFNULL(beh.cnt_ad_reward, 0) AS FLOAT64) AS cnt_ad_reward,\n",
    "    CAST(IFNULL(beh.cnt_challenge_a_friend, 0) AS FLOAT64) AS cnt_challenge_a_friend,\n",
    "    CAST(IFNULL(beh.cnt_completed_5_levels, 0) AS FLOAT64) AS cnt_completed_5_levels,\n",
    "    CAST(IFNULL(beh.cnt_use_extra_steps, 0) AS FLOAT64) AS cnt_use_extra_steps,\n",
    "    ret.churned as churned,\n",
    "    CASE\n",
    "      WHEN churned = 0 THEN ( SELECT class_weight_zero FROM get_class_weights)\n",
    "      ELSE ( SELECT class_weight_one\n",
    "       FROM get_class_weights)\n",
    "    END AS class_weights\n",
    "FROM\n",
    "  get_label ret\n",
    "LEFT OUTER JOIN\n",
    "  get_demographic_data dem\n",
    "ON \n",
    "  ret.user_pseudo_id = dem.user_pseudo_id\n",
    "LEFT OUTER JOIN \n",
    "  get_behavioral_data beh\n",
    "ON\n",
    "  ret.user_pseudo_id = beh.user_pseudo_id\n",
    "WHERE ret.bounced = 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Z6CjIOmDsET-"
   },
   "outputs": [],
   "source": [
    "run_bq_query(preprocess_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrEKovOhCrUq"
   },
   "source": [
    "### Create table to update entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26db23c2242a"
   },
   "outputs": [],
   "source": [
    "processed_sql_query_day_one = f\"\"\"\n",
    "CREATE OR REPLACE TABLE \n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_TODAY}` AS\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}`\n",
    "WHERE\n",
    "    user_first_engagement < '{TODAY}'\n",
    "\"\"\"\n",
    "\n",
    "processed_sql_query_day_two = f\"\"\"\n",
    "CREATE OR REPLACE TABLE \n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_TOMORROW}` AS\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}`\n",
    "WHERE\n",
    "  user_first_engagement >= '{TODAY}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AOgLg0HDETq"
   },
   "outputs": [],
   "source": [
    "queries = processed_sql_query_day_one, processed_sql_query_day_two\n",
    "for query in queries:\n",
    "    run_bq_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx__2-assET-"
   },
   "source": [
    "## Set up a Vertex AI Feature store\n",
    "\n",
    "Now we have the wide table of features. It is time to ingest them into the feature store. As you can see in the picture below, Vertex AI Feature Store organizes resources hierarchically in the following order: `Featurestore -> EntityType -> Feature`. You must create these resources before you can ingest data into Vertex AI Feature Store.\n",
    "\n",
    "In our case we are going to create **mobile_gaming** featurestore resource containing **user** entity type and all its associated **features** such as country or the number of times a user challenged a friend (cnt_challenge_a_friend).\n",
    "\n",
    "<img src=\"./assets/3_feature_store_datamodel.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNlxda2sET_"
   },
   "source": [
    "### Create featurestore, ```mobile_gaming```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfBpw2J5P1sa",
    "outputId": "15c30a67-8e1e-498a-83cc-7cf28401a6cb"
   },
   "outputs": [],
   "source": [
    "print(f\"Listing all featurestores in {PROJECT_ID}\")\n",
    "feature_store_list = Featurestore.list()\n",
    "if len(list(feature_store_list)) == 0:\n",
    "    print(f\"The {PROJECT_ID} is empty!\")\n",
    "else:\n",
    "    for fs in feature_store_list:\n",
    "        print(\"Found featurestore: {}\".format(fs.resource_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2en8I7TSe4b",
    "outputId": "8929514e-b810-49b3-ce4c-9911bf42b0b6"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mobile_gaming_feature_store = Featurestore.create(\n",
    "        featurestore_id=FEATURESTORE_ID,\n",
    "        online_store_fixed_node_count=ONLINE_STORE_NODES_COUNT,\n",
    "        labels={\"team\": \"dataoffice\", \"app\": \"mobile_gaming\"},\n",
    "        sync=True,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    FEATURESTORE_RESOURCE_NAME = mobile_gaming_feature_store.resource_name\n",
    "    print(f\"Feature store created: {FEATURESTORE_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN-vlvPUsET_"
   },
   "source": [
    "### Create the ```User``` entity type and its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbZ2RQ5XbuRq",
    "outputId": "1ece3ddb-a14f-4a1c-f49c-f88fe8e7a4e0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type = mobile_gaming_feature_store.create_entity_type(\n",
    "        entity_type_id=ENTITY_ID, description=\"User Entity\", \n",
    "        sync=True\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    USER_ENTITY_RESOURCE_NAME = user_entity_type.resource_name\n",
    "    print(\"Entity type name is\", USER_ENTITY_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2PIAprPmnhB"
   },
   "source": [
    "### Set Feature Monitoring\n",
    "\n",
    "Feature [monitoring](https://cloud.google.com/vertex-ai/docs/featurestore/monitoring) is in preview, so you need to use v1beta1 Python which is a lower-level API than the one we've used so far in this notebook. \n",
    "\n",
    "The easiest way to set this for now is using [console UI](https://console.cloud.google.com/vertex-ai/features). For completeness, below is example to do this using v1beta1 SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6im2c3ymiwC",
    "outputId": "88d07c18-e45c-4b4f-dd84-33aaf4dd56bf"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1beta1 import \\\n",
    "    FeaturestoreServiceClient as v1beta1_FeaturestoreServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    entity_type as v1beta1_entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_monitoring as v1beta1_featurestore_monitoring_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_service as v1beta1_featurestore_service_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "v1beta1_admin_client = v1beta1_FeaturestoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp9xaLQXn0CS",
    "outputId": "2af9857b-4775-4bdf-da6b-a672d10e0fd9"
   },
   "outputs": [],
   "source": [
    "v1beta1_admin_client.update_entity_type(\n",
    "    v1beta1_featurestore_service_pb2.UpdateEntityTypeRequest(\n",
    "        entity_type=v1beta1_entity_type_pb2.EntityType(\n",
    "            name=v1beta1_admin_client.entity_type_path(\n",
    "                PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_ID\n",
    "            ),\n",
    "            monitoring_config=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                snapshot_analysis=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                    monitoring_interval=Duration(seconds=86400),  # 1 day\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ustwKOMle8Qp"
   },
   "source": [
    "### Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijeZCTKIfCRL"
   },
   "source": [
    "#### Create Feature configuration\n",
    "\n",
    "For simplicity, I created the configuration in a declarative way. Of course, we can create an helper function to built it from Bigquery schema.\n",
    "Also notice that we want to pass some feature on-fly. In this case, it country, operating system and language looks perfect for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX_uYmjUgd9x"
   },
   "outputs": [],
   "source": [
    "feature_configs = {\n",
    "    \"country\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The country of customer\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The operating system of device\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"language\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The language of device\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_user_engagement\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_start_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with start level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_end_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with end level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_complete_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with complete status\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_reset_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with reset status\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_post_score\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user score\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_spend_virtual_currency\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user virtual amount\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_ad_reward\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user reward\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_challenge_a_friend\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user challenges with friends\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_completed_5_levels\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user level 5 completed\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_use_extra_steps\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user extra steps\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"class_weights\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"A variable of class weights\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErkruXPJkPuy"
   },
   "source": [
    "#### Create features using `batch_create_features` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsCAO_IfsEUC",
    "outputId": "024b2603-02df-4ee6-b58f-920839051038"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type.batch_create_features(feature_configs=feature_configs, sync=True)\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    for feature in user_entity_type.list_features():\n",
    "        print(\"\")\n",
    "        print(f\"The resource name of {feature.name} feature is\", feature.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WisJk18qqgs"
   },
   "source": [
    "### Search features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqyarMZqvZS",
    "outputId": "36651139-8d3d-4406-cdd3-cf45b99b20e2"
   },
   "outputs": [],
   "source": [
    "feature_query = \"feature_id:cnt_user_engagement\"\n",
    "searched_features = Feature.search(query=feature_query)\n",
    "searched_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugtBfW5gsEUD"
   },
   "source": [
    "## Ingest features \n",
    "\n",
    "You need to import feature values before you can use them for online/offline serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QgjW2vhsEUE"
   },
   "outputs": [],
   "source": [
    "FEATURES_IDS = [feature.name for feature in user_entity_type.list_features()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwltj62V8gTU",
    "outputId": "1bb25edf-cd69-4164-9dfa-3a6a11f63a12"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type.ingest_from_bq(\n",
    "        feature_ids=FEATURES_IDS,\n",
    "        feature_time=FEATURE_TIME,\n",
    "        bq_source_uri=BQ_SOURCE_URI_TODAY,\n",
    "        entity_id_field=ENTITY_ID_FIELD,\n",
    "        disable_online_serving=False,\n",
    "        worker_count=20,\n",
    "        sync=True,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Yv8MenWXrRX"
   },
   "source": [
    "# Train a churn ML model using Vertex AI Feature Store and Training\n",
    "\n",
    "Now that we have features, we can train our churn model.\n",
    "\n",
    "<img src=\"./assets/4_train_model.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2s3Nk5UuAvd"
   },
   "source": [
    "**Comment: How does Vertex AI Feature Store mitigate training serving skew?**\n",
    "\n",
    "Let's just think about what is happening for a second. \n",
    "\n",
    "We just ingest customer behavioral features we engineered. And we are going to serve the same features for online prediction.\n",
    "\n",
    "But, what if those attributes on the incoming prediction requests would differ with respect to the one calculated during the model training? In particular, what if the correct attributes have different characteristics as the data the model was trained on? At that point, you should start perceiving this idea of **skew** between training and serving data.\n",
    "\n",
    "**Vertex AI Feature store** addresses those skew by an ingest-one and and re-used many logic. Indeed, once the feature is computed, the same features would be available both in training and serving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saZZ3zWKX1YK"
   },
   "source": [
    "## Avoid data leakage with point-in-time lookup to fetch training data\n",
    "\n",
    "Now, without a datastore with a timestamp data model, some data leakage would happen and you would end by training the new model on a different dataset. As a consequence, you cannot compare those models. In order to avoid that, **you need to be able to train model on the same data at same specific point in time we use in the previous version of the model**. \n",
    "\n",
    "**With the Vertex AI Feature store, you can fetch feature values corresponding to a particular timestamp thanks to point-in-time lookup capability.** In terms of SDK, you need to define a `read instances` object which is a list of entity id / timestamp pairs, where the entity id is the `user_pseudo_id` and `user_first_engagement` indicates we want to read the latest information available about that user. In this way, we will be able to reproduce the exact same training sample you need for the new model.\n",
    "\n",
    "Let's see how to do that. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHNbIHqFcQiM"
   },
   "source": [
    "### Define query for reading instances at a specific point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QntX3uQ93ysM"
   },
   "outputs": [],
   "source": [
    "# WHERE ABS(MOD(FARM_FINGERPRINT(STRING(user_first_engagement, 'UTC')), 10)) < 8\n",
    "\n",
    "read_instances_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}` AS\n",
    "    SELECT\n",
    "      user_pseudo_id  as user,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CONCAT('2018-10-04', ' ', STRING(TIME_TRUNC(CURRENT_TIME(), SECOND))), 'UTC') as timestamp,\n",
    "      churned,\n",
    "    FROM\n",
    "      `{BQ_DATASET}.{FEATURES_TABLE_DAY_ONE}` AS e\n",
    "    ORDER BY\n",
    "      user_first_engagement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTB3853wcYm6"
   },
   "source": [
    "### Create the BigQuery instances table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PRgsZl09vJ0"
   },
   "outputs": [],
   "source": [
    "run_bq_query(read_instances_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwSsdk2AcdTf"
   },
   "source": [
    "### Serve features for batch training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnP0Q5XtwfYM",
    "outputId": "be0491ac-c851-4212-925b-fdb0be0f1d41"
   },
   "outputs": [],
   "source": [
    "mobile_gaming_feature_store.batch_serve_to_gcs(\n",
    "    gcs_destination_output_uri_prefix=GCS_DESTINATION_OUTPUT_URI,\n",
    "    gcs_destination_type = 'csv',\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=READ_INSTANCES_URI,\n",
    "    pass_through_fields=['churned']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjvv7NKnYfcU"
   },
   "source": [
    "## Train and deploy a custom model on Vertex AI with Training Pipelines\n",
    "\n",
    "Now that we reproduce the training sample, we use the Vertex AI SDK to train an new version of the model using Vertex AI Training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAYW_N_Ewc_5"
   },
   "outputs": [],
   "source": [
    "!rm -Rf train_package #if train_package already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZLVGnWzBVER",
    "outputId": "961ee41f-bd92-46ab-9d3c-eac10c850e3d"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p trainer data/ingest data/raw model config\n",
    "!gsutil -m cp -r $GCS_DESTINATION_OUTPUT_URI/*.csv data/ingest\n",
    "!head -n 1000 data/ingest/*.csv > data/raw/sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLmh54sm8m0l"
   },
   "source": [
    "### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTp85Qfpwc_6"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVGEf68fDSax"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Get arguments from command line.\n",
    "    Returns:\n",
    "        args: parsed arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--data_path',\n",
    "        required=False,\n",
    "        default=os.getenv('AIP_TRAINING_DATA_URI'),\n",
    "        type=str,\n",
    "        help='path to read data')\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        required=False,\n",
    "        default=0.01,\n",
    "        type=int,\n",
    "        help='number of epochs')\n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        required=False,\n",
    "        default=os.getenv('AIP_MODEL_DIR'),\n",
    "        type=str,\n",
    "        help='dir to store saved model')\n",
    "    parser.add_argument(\n",
    "        '--config_path',\n",
    "        required=False,\n",
    "        default='../config.yaml',\n",
    "        type=str,\n",
    "        help='path to read config file')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def ingest_data(data_path, data_model_params):\n",
    "    \"\"\"\n",
    "    Ingest data\n",
    "    Args:\n",
    "        data_path: path to read data\n",
    "        data_model_params: data model parameters\n",
    "    Returns:\n",
    "        df: dataframe\n",
    "    \"\"\"\n",
    "    # read training data\n",
    "    df = pd.read_csv(data_path, sep=',',\n",
    "                     dtype={col: 'string' for col in data_model_params['categorical_features']})\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df, data_model_params):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    Args:\n",
    "        df: dataframe\n",
    "        data_model_params: data model parameters\n",
    "    Returns:\n",
    "        df: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # convert nan values because pd.NA ia not supported by SimpleImputer\n",
    "    # bug in sklearn 0.23.1 version: https://github.com/scikit-learn/scikit-learn/pull/17526\n",
    "    # decided to skip NAN values for now\n",
    "    df.replace({pd.NA: np.nan}, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # get features and labels\n",
    "    x = df[data_model_params['numerical_features'] + data_model_params['categorical_features'] + [\n",
    "        data_model_params['weight_feature']]]\n",
    "    y = df[data_model_params['target']]\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                        test_size=data_model_params['train_test_split']['test_size'],\n",
    "                                                        random_state=data_model_params['train_test_split'][\n",
    "                                                            'random_state'])\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_pipeline(learning_rate, model_params):\n",
    "    \"\"\"\n",
    "    Build pipeline\n",
    "    Args:\n",
    "        learning_rate: learning rate\n",
    "        model_params: model parameters\n",
    "    Returns:\n",
    "        pipeline: pipeline\n",
    "    \"\"\"\n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        # ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', xgb.XGBClassifier(learning_rate=learning_rate,\n",
    "                                    use_label_encoder=False, #deprecated and breaks Vertex AI predictions\n",
    "                                    **model_params))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Starting training...')\n",
    "    args = get_args()\n",
    "    data_path = args.data_path\n",
    "    learning_rate = args.learning_rate\n",
    "    model_dir = args.model_dir\n",
    "    config_path = args.config_path\n",
    "\n",
    "    # read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    f.close()\n",
    "    data_model_params = config['data_model_params']\n",
    "    model_params = config['model_params']\n",
    "\n",
    "    # ingest data\n",
    "    print('Reading data...')\n",
    "    data_df = ingest_data(data_path, data_model_params)\n",
    "\n",
    "    # preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    x_train, x_test, y_train, y_test = preprocess_data(data_df, data_model_params)\n",
    "    sample_weight = x_train.pop(data_model_params['weight_feature'])\n",
    "    sample_weight_eval_set = x_test.pop(data_model_params['weight_feature'])\n",
    "\n",
    "    # train lgb model\n",
    "    print('Training model...')\n",
    "    xgb_pipeline = build_pipeline(learning_rate, model_params)\n",
    "    # need to use fit_transform to get the encoded eval data\n",
    "    x_train_transformed = xgb_pipeline[:-1].fit_transform(x_train)\n",
    "    x_test_transformed = xgb_pipeline[:-1].transform(x_test)\n",
    "    xgb_pipeline[-1].fit(x_train_transformed, y_train,\n",
    "                         sample_weight=sample_weight,\n",
    "                         eval_set=[(x_test_transformed, y_test)],\n",
    "                         sample_weight_eval_set=[sample_weight_eval_set],\n",
    "                         eval_metric='error',\n",
    "                         early_stopping_rounds=50,\n",
    "                         verbose=True)\n",
    "    # save model\n",
    "    print('Saving model...')\n",
    "    model_path = Path(model_dir)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(xgb_pipeline, f'{model_dir}/model.joblib')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD28QoIw8sdf"
   },
   "source": [
    "### Create requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZfnfDAH8yos"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "pip==22.0.4\n",
    "PyYAML==5.3.1\n",
    "joblib==0.15.1\n",
    "numpy==1.18.5\n",
    "pandas==1.0.4\n",
    "scipy==1.4.1\n",
    "scikit-learn==0.23.1\n",
    "xgboost==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3KLInhPwc_7"
   },
   "source": [
    "### Create training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E7dPiChwc_7"
   },
   "outputs": [],
   "source": [
    "%%writefile config/config.yaml\n",
    "data_model_params:\n",
    "  target: churned\n",
    "  categorical_features:\n",
    "    - country\n",
    "    - operating_system\n",
    "    - language\n",
    "  numerical_features:\n",
    "    - cnt_user_engagement\n",
    "    - cnt_level_start_quickplay\n",
    "    - cnt_level_end_quickplay\n",
    "    - cnt_level_complete_quickplay\n",
    "    - cnt_level_reset_quickplay\n",
    "    - cnt_post_score\n",
    "    - cnt_spend_virtual_currency\n",
    "    - cnt_ad_reward\n",
    "    - cnt_challenge_a_friend\n",
    "    - cnt_completed_5_levels\n",
    "    - cnt_use_extra_steps\n",
    "  weight_feature: class_weights\n",
    "  train_test_split:\n",
    "    test_size: 0.2\n",
    "    random_state: 8\n",
    "model_params:\n",
    "  booster: gbtree\n",
    "  objective: binary:logistic\n",
    "  max_depth: 80\n",
    "  n_estimators: 100\n",
    "  random_state: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55WduJ3M9WuH"
   },
   "source": [
    "### Test the model locally with `local-run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKMtFOTM9R63"
   },
   "outputs": [],
   "source": [
    "test_job_script=f\"\"\"\n",
    "gcloud ai custom-jobs local-run \\\n",
    "--executor-image-uri={BASE_CPU_IMAGE} \\\n",
    "--python-module=trainer.task \\\n",
    "--extra-dirs=config,data,model \\\n",
    "-- \\\n",
    "--data_path data/raw/sample.csv \\\n",
    "--model_dir model \\\n",
    "--config_path config/config.yaml\n",
    "\"\"\"\n",
    "\n",
    "with open('local_train_job_run.sh', 'w+') as s:\n",
    "    s.write(test_job_script)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9teD2VCAwc_7"
   },
   "outputs": [],
   "source": [
    "!chmod +x ./local_train_job_run.sh && ./local_train_job_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z1Kwg6pe0Jx"
   },
   "source": [
    "### Create and Launch the Custom training pipeline to train the model with `autopackaging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcnEOZKywc_8"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p {MODEL_PACKAGE_PATH} && mv -t {MODEL_PACKAGE_PATH} trainer requirements.txt config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ4koRVCwc_8"
   },
   "outputs": [],
   "source": [
    "train_job_script=f\"\"\"\n",
    "gcloud ai custom-jobs create \\\n",
    "--region={REGION} \\\n",
    "--display-name={TRAIN_JOB_NAME} \\\n",
    "--worker-pool-spec=machine-type={TRAINING_MACHINE_TYPE},replica-count={TRAINING_REPLICA_COUNT},executor-image-uri={BASE_CPU_IMAGE},local-package-path={MODEL_PACKAGE_PATH},python-module=trainer.task,extra-dirs=config \\\n",
    "--args=--data_path={DATA_PATH},--model_dir={MODEL_DIR},--config_path=config/config.yaml \\\n",
    "--verbosity='info'\n",
    "\"\"\"\n",
    "\n",
    "with open('train_job_run.sh', 'w+') as s:\n",
    "    s.write(train_job_script)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwq1Jyklwc_8"
   },
   "outputs": [],
   "source": [
    "!chmod +x ./train_job_run.sh && ./train_job_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpOA6aw5wc_8"
   },
   "outputs": [],
   "source": [
    "!gcloud ai custom-jobs describe projects/309823771116/locations/us-central1/customJobs/7454619017332916224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $DESTINATION_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c9330928aa1"
   },
   "source": [
    "# Serve ML features at scale with low latency\n",
    "\n",
    "At that point, **we deploy our simple model which would requires fetching aggregated attributes as input features in real time**. \n",
    "\n",
    "That's why **we need a datastore optimized for singleton lookup operations** which would be able to scale and serve those aggregated feature online in low latency. \n",
    "\n",
    "In other terms, we need to introduce Vertex AI Feature Store. Again, we assume you already know how to set up and work with a Vertex AI Feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC9fkIMBwc_8"
   },
   "source": [
    "### Upload and Deploy Model on Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W17gaIjtwc_8"
   },
   "outputs": [],
   "source": [
    "xgb_model = upload_model(\n",
    "    display_name=MODEL_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    artifact_uri=DESTINATION_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWZmWjnye4N5"
   },
   "source": [
    "### Deploy Model to the same Endpoint with Traffic Splitting\n",
    "\n",
    "Vertex AI Endpoint provides a managed traffic splitting service. All you need to do is to define the splitting policy and then the service will deal it for you. \n",
    "\n",
    "Be sure that both models have the same serving function. In our case both BQML Logistic classifier and Vertex AI AutoML support same prediction format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWpDZVtmwc_9"
   },
   "outputs": [],
   "source": [
    "endpoint = create_endpoint(display_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Nw4o2fJwc_9"
   },
   "outputs": [],
   "source": [
    "deployed_model = deploy_model(\n",
    "    model=xgb_model,\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6V2-YQthT4V"
   },
   "source": [
    "## Simulate online prediction requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVrvAW5VaPzU"
   },
   "outputs": [],
   "source": [
    "online_sample = {\n",
    "    \"entity_id\": [\"DE346CDD4A6F13969F749EA8047F282A\"],\n",
    "    \"country\": [\"United States\"],\n",
    "    \"operating_system\": [\"IOS\"],\n",
    "    \"language\": [\"en\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUku2KDfHJxC"
   },
   "outputs": [],
   "source": [
    "prediction = simulate_prediction(endpoint=endpoint, online_sample=online_sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e600b031a50"
   },
   "source": [
    "## Time to simulate online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k42QhcjcG5pe"
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    simulate_prediction(endpoint=endpoint, online_sample=online_sample)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fb651556252"
   },
   "source": [
    "Below the Vertex AI Endpoint UI result you would able to see after the online prediction simulation ends\n",
    "\n",
    "<img src=\"./assets/prediction_results.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDlkhkCPK29y"
   },
   "source": [
    "## Ingest new data in the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-_w9IRCQTV9",
    "outputId": "60ba7e65-511b-49c9-edfe-0d16d2153a40"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    user_entity_type.ingest_from_bq(\n",
    "        feature_ids=FEATURES_IDS,\n",
    "        feature_time=FEATURE_TIME,\n",
    "        bq_source_uri=BQ_SOURCE_URI_TOMORROW,\n",
    "        entity_id_field=ENTITY_ID_FIELD,\n",
    "        disable_online_serving=False,\n",
    "        worker_count=5,\n",
    "        sync=True,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NESfOgK5nkNu",
    "outputId": "54df5c33-a9c5-4613-f006-2f65bd6ad826"
   },
   "outputs": [],
   "source": [
    "# delete feature store\n",
    "mobile_gaming_feature_store.delete(sync=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W28-97kfPDb"
   },
   "outputs": [],
   "source": [
    "# delete Vertex AI resources\n",
    "endpoint.undeploy_all()\n",
    "xgb_model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Warning: Setting this to true will delete everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket and \"BUCKET_URI\" in globals():\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLVhx7bQeJ6H"
   },
   "outputs": [],
   "source": [
    "# Delete the BigQuery Dataset\n",
    "!bq rm -r -f -d $PROJECT_ID:$BQ_DATASET"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mobile_gaming_feature_store.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
